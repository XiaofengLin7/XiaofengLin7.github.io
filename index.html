<!DOCTYPE HTML>
<meta http-equiv='cache-control' content='no-cache'> 
<meta http-equiv='expires' content='0'> 
<meta http-equiv='pragma' content='no-cache'>

<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Xiaofeng Lin</title>

    <meta name="author" content="Xiaofeng Lin">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="images/humanoid-svgrepo-com.ico">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Xiaofeng Lin
                </p>
                <p>I am a third-year Systems Engineering Ph.D. student at <a href="https://www.bu.edu/homepage-alt/">Boston University</a> where I work on reinforcement learning under the supervision of <a href="https://zhangxz1123.github.io/">Prof. Xuezhou Zhang</a>.
                </p>
                <p>
                  I earned my Master of Science in Robotics from <a href= "https://robotics.umich.edu/">University of Michigan, Ann Arbor</a>, where I was fortunate to be advised by <a href="https://vasileiostzoumas.com/">Prof. Vasileios Tzoumas</a>. During my master, I worked on online learnig, submodular maximization and multi-agent systems.
                </p>
                <p>
                  I earned my B.S.E in Engineering Mechanics from <a href= "http://www.tju.edu.cn/english/index.html">Tianjin University</a>.
                </p>
                <p style="text-align:center">
                  <a href="data/Resume of Xiaofeng Lin07092024.pdf">Resume</a> &nbsp;/&nbsp;
                  <a href="mailto:xfl@bu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=Fshb3ykAAAAJ&hl=en">Google Scholar</a>&nbsp;/&nbsp;
                  <a href="https://github.com/XiaofengLin7">Github</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/Lin1420623">X</a> 
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/Xiaofeng_Lin.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Xiaofeng_Lin.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm broadly interested in reinforcement learning, post-training of LLMs and LLM agents. I aspire to make LLMs and LLM agents make better decisions in different domains.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
              
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Debunk the Myth of SFT Generalization</span>
                <br>
                <strong>Xiaofeng Lin*</strong>, Hejian Sang*, Zhipeng Wang, Xuezhou Zhang
                <br>
                <em>preprint, 2025.</em>
                <br>
                <a href="https://arxiv.org/abs/2510.00237">arXiv</a> / <a href="https://github.com/XiaofengLin7/debunking-sft-generalization">code</a>
                <p></p>
                <p>This paper challenges the view that SFT fails to generalize and shows that with prompt diversity and chain-of-thought supervision, SFT can match or surpass RL baselines on decision-making tasks.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='sim_real_image'>
                  <source src="images/2025AAAI.png">
                 </div>
                  <img src='images/2025AAAI.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Efficient Reinforcement Learning in Probabilistic Reward Machines</span>
                <br> 
                <strong>Xiaofeng Lin</strong>, <a href="https://zhangxz1123.github.io/">Xuezhou Zhang</a>
                <br>
                <em>AAAI, 2025. <span style="color: red;">(Oral)</span></em>
                <br>
                <a href="https://arxiv.org/abs/2408.10381">arXiv</a> /
                <a href="https://github.com/XiaofengLin7/UCBVI_PRM">code</a> /
                <a href="data/AAAI_2025_slides.pdf">slides</a> 
                <p></p>
                <p>This paper studies how to explore given the knowledge of Probabilistic Reward Machines (PRMs) and extends reward-free framework to generic non-Markovian rewards setting.</p>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='sim_real_image'>
                  <source src="images/Unmanned_Systems_2024.png">
                 </div>
                  <img src='images/Unmanned_Systems_2024.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">A Real-to-Sim-to-Real Approach for Vision-Based Autonomous MAV-Catching-MAV</span>
                <br>
                
        Zian Ning, Yin Zhang,  
                <strong>Xiaofeng Lin</strong>, <a href="https://shiyuzhao.westlake.edu.cn/">Shiyu Zhao</a>
                <br>
                <em>Unmanned Systems, 2024.</em>
                <br>
                <a href="https://www.worldscientific.com/doi/10.1142/S2301385025500360">World Scientific</a> / <a href="data/202405ningzian.pdf">PDF</a>
                <p></p>
                <p>This paper studies the task of vision-based MAV-catching-MAV, where a catcher MAV can detect, localize, and pursue a target MAV autonomously. This paper proposes a real-to-sim-to-real approach and sucessfully implements a fully autonomous vision-based MAV-catching-MAV system.</p>
              </td>
            </tr>
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
            <div class="two" id='metabsg_image'>
            <source src="images/partial_observable (1).png">
           </div>
            <img src='images/partial_observable (1).png' width="160">
          </div>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Leveraging Untrustworthy Commands for Multi-Robot Coordination in Unpredictable Environments: A Bandit Submodular Maximization Approach</span>
          <br>
          
  <a href="https://zirui-xu.github.io/">Zirui Xu*</a>, 
          <strong>Xiaofeng Lin*</strong>, <a href="https://vasileiostzoumas.com">Vasileios Tzoumas</a>
          <br>
          <em>American Control Conference (ACC), 2024.</em>
          <br>
          <a href="https://arxiv.org/abs/2309.16161">arXiv</a> /
          <a href="https://github.com/UM-iRaL/Meta-Bandit-Sequential-Greedy">code</a> 
          <p></p>
          <p>The algorithm leverages a meta-algorithm to learn whether the robots should follow untrustworthy commands or a recently developed submodular coordination algorithm, Bandit Sequential Greedy (BSG), which has performance guarantees. The algorithm asymptotically can achieve the better performance out of the commands and the BSG algorithm.</p>
        </td>
      </tr>
        <tr>
        <td style="padding:20px;width:25%;vertical-align:middle" onmouseout="bsg_stop()" onmouseover="bsg_start()">
          <div class="one">
            <div class="two" id='bsg_image'><video  width=100% height=100% muted loop>
            <source src="images/BSG_2v2_adversarial_2X.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/BSG_2v2_adversarial_2X.png' width="160">
          </div>
          <script type="text/javascript">
            function bsg_start() {
                document.getElementById('bsg_image').querySelector('video').play();
              }

            function bsg_stop() {
                document.getElementById('bsg_image').querySelector('video').pause();
              } 
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <span class="papertitle">Bandit Submodular Maximization for Multi-Robot Coordination in Unpredictable and Partially Observable Environments</span>
          <br>
          
  <a href="https://zirui-xu.github.io/">Zirui Xu</a>, 
          <strong>Xiaofeng Lin</strong>, <a href="https://vasileiostzoumas.com">Vasileios Tzoumas</a>
          <br>
          <em>Robotics: Science and Systems (RSS), 2023.</em>
          <br>
          <a href="https://arxiv.org/abs/2305.12795">arXiv</a> /
          <a href="https://github.com/UM-iRaL/bandit-sequential-greedy">code</a> /
          <a href="https://drive.google.com/drive/folders/1_JbiEUEHabiIm_J7ZoLqCwsjtA_cSIkS">simulation videos</a> /
          <a href="https://www.youtube.com/watch?v=CrRC7AznsqE">presentation</a>
          <p></p>
          <p>The algorithm generalizes the seminal Sequential Greedy algorithm by Fisher et al. to the bandit setting, by leveraging submodularity and algorithms for the problem of tracking the best action. We validate our algorithm in simulated scenarios of multi-target tracking.</p>
        </td>
      </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <h2>Teaching Assistant</h2>
              <ul>
                <li> BU SE 524/674: Optimization Theory and Methods (Fall 2024)</li>
                <li>BU EK 125: Introduction to Programming for Engineers (Fall 2025)</li>
              </ul>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <h2>Service</h2>
            <ul>
              <li> Reviewer: ACC, ICLR, NeurIPS, AISTATS, ICML.</li>
            </ul>
          </td>
        </tr>
      </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Thanks for <a ref="https://jonbarron.info/">Jon Barron</a> for this amazing template.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
